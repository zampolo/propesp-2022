% Encoding: UTF-8
@article{jegham-2020,
	title = {Vision-based human action recognition: An overview and real world challenges},
	volume = {32},
	issn = {26662817},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S174228761930283X},
	doi = {10.1016/j.fsidi.2019.200901},
	shorttitle = {Vision-based human action recognition},
	abstract = {Within a large range of applications in computer vision, Human Action Recognition has become one of the most attractive research ﬁelds. Ambiguities in recognizing actions does not only come from the difﬁculty to deﬁne the motion of body parts, but also from many other challenges related to real world problems such as camera motion, dynamic background, and bad weather conditions. There has been little research work in the real world conditions of human action recognition systems, which encourages us to seriously search in this application domain. Although a plethora of robust approaches have been introduced in the literature, they are still insufﬁcient to fully cover the challenges. To quantitatively and qualitatively compare the performance of these methods, public datasets that present various actions under several conditions and constraints are recorded. In this paper, we investigate an overview of the existing methods according to the kind of issue they address. Moreover, we present a comparison of the existing datasets introduced for the human action recognition ﬁeld.},
	pages = {200901},
	journaltitle = {Forensic Science International: Digital Investigation},
	shortjournal = {Forensic Science International: Digital Investigation},
	author = {Jegham, Imen and Ben Khalifa, Anouar and Alouani, Ihsen and Mahjoub, Mohamed Ali},
	urldate = {2020-07-02},
	date = {2020-03},
	langid = {english},
	keywords = {done, selected, review},
	file = {Jegham et al. - 2020 - Vision-based human action recognition An overview.pdf:/home/zampolo/Zotero/storage/BHU3L7TY/Jegham et al. - 2020 - Vision-based human action recognition An overview.pdf:application/pdf}
}

@article{hussain-2020,
	title = {A review and categorization of techniques on device-free human activity recognition},
	volume = {167},
	issn = {10848045},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1084804520302125},
	doi = {10.1016/j.jnca.2020.102738},
	abstract = {Human activity recognition has gained importance in recent years due to its applications in various ﬁelds such as health, security and surveillance, entertainment, and intelligent environments. A signiﬁcant amount of work has been done on human activity recognition and researchers have leveraged diﬀerent approaches, such as wearable, object-tagged, and device-free, to recognize human activities. In this article, we present a comprehensive survey of the work conducted over the 10-year period of 2010–2019 in various areas of human activity recognition with main focus on device-free solutions. The device-free approach is becoming very popular due to the fact that the subject is not required to carry anything. Instead, the environment is tagged with devices to capture the required information. We propose a new taxonomy for categorizing the research work conducted in the ﬁeld of activity recognition and divide the existing literature into three sub-areas: action-based, motion-based, and interactionbased. We further divide these areas into ten diﬀerent sub-topics and present the latest research works in these sub-topics. Unlike previous surveys which focus only on one type of activities, to the best of our knowledge, we cover all the sub-areas in activity recognition and provide a comparison of the latest research work in these subareas. Speciﬁcally, we discuss the key attributes and design approaches for the work presented. Then we provide extensive analysis based on 10 important metrics, to present a comprehensive overview of the state-of-the-art techniques and trends in diﬀerent sub-areas of device-free human activity recognition. In the end, we discuss open research issues and propose future research directions in the ﬁeld of human activity recognition.},
	pages = {102738},
	journaltitle = {Journal of Network and Computer Applications},
	shortjournal = {Journal of Network and Computer Applications},
	author = {Hussain, Zawar and Sheng, Quan Z. and Zhang, Wei Emma},
	urldate = {2020-07-02},
	date = {2020-10},
	langid = {english},
	keywords = {done, selected, review},
	file = {Hussain et al. - 2020 - A review and categorization of techniques on devic.pdf:/home/zampolo/Zotero/storage/PZCGRYEW/Hussain et al. - 2020 - A review and categorization of techniques on devic.pdf:application/pdf}
}

@article{yao-2019,
	title = {A review of Convolutional-Neural-Network-based action recognition},
	volume = {118},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865518302058},
	doi = {10.1016/j.patrec.2018.05.018},
	abstract = {Video action recognition is widely applied in video indexing, intelligent surveillance, multimedia understanding, and other ﬁelds. Recently, it was greatly improved by incorporating the learning of deep information using Convolutional Neural Network ({CNN}). This motivated us to review the notable {CNN}-based action recognition works. Because {CNN} is primarily designed to extract 2D spatial features from still image and videos are naturally viewed as 3D spatiotemporal signals, the core issue of extending the {CNN} from image to video is temporal information exploitation. We divide the solutions for exploiting temporal information exploration into three strategies: 1) 3D {CNN}; 2) taking the motion-related information as the {CNN} input; and 3) fusion. In this paper, we present a comprehensive review of the {CNN}-based action recognition methods according to these strategies. We also discuss the action recognition performance on recent large-scale benchmarks and the limitations and future research directions of {CNN}-based action recognition. This paper offers an objective and clear review of {CNN}-based action recognition and provides a guide for future research.},
	pages = {14--22},
	journaltitle = {Pattern Recognition Letters},
	shortjournal = {Pattern Recognition Letters},
	author = {Yao, Guangle and Lei, Tao and Zhong, Jiandan},
	urldate = {2020-07-21},
	date = {2019-02},
	langid = {english},
	keywords = {done, selected, review},
	file = {Yao et al. - 2019 - A review of Convolutional-Neural-Network-based act.pdf:/home/zampolo/Zotero/storage/UTAMVJM9/Yao et al. - 2019 - A review of Convolutional-Neural-Network-based act.pdf:application/pdf}
}

@article{kongr-2018,
	title = {Human Action Recognition and Prediction: A Survey},
	url = {http://arxiv.org/abs/1806.11230},
	shorttitle = {Human Action Recognition and Prediction},
	abstract = {Derived from rapid advances in computer vision and machine learning, video analysis tasks have been moving from inferring the present state to predicting the future state. Vision-based action recognition and prediction from videos are such tasks, where action recognition is to infer human actions (present state) based upon complete action executions, and action prediction to predict human actions (future state) based upon incomplete action executions. These two tasks have become particularly prevalent topics recently because of their explosively emerging real-world applications, such as visual surveillance, autonomous driving vehicle, entertainment, and video retrieval, etc. Many attempts have been devoted in the last a few decades in order to build a robust and effective framework for action recognition and prediction. In this paper, we survey the complete state-of-the-art techniques in the action recognition and prediction. Existing models, popular algorithms, technical difﬁculties, popular action databases, evaluation protocols, and promising future directions are also provided with systematic discussions.},
	journaltitle = {{arXiv}:1806.11230 [cs]},
	author = {Kong, Yu and Fu, Yun},
	urldate = {2020-06-24},
	date = {2018-07-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1806.11230},
	keywords = {done, selected, review},
	file = {1806.11230(1).pdf:/home/zampolo/engenharia/artigos/activity-recognition/1806.11230(1).pdf:application/pdf}
}

@article{herath-2017,
	title = {Going deeper into action recognition: A survey},
	volume = {60},
	issn = {02628856},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0262885617300343},
	doi = {10.1016/j.imavis.2017.01.010},
	shorttitle = {Going deeper into action recognition},
	abstract = {Understanding human actions in visual data is tied to advances in complementary research areas including object recognition, human dynamics, domain adaptation and semantic segmentation. Over the last decade, human action analysis evolved from earlier schemes that are often limited to controlled environments to nowadays advanced solutions that can learn from millions of videos and apply to almost all daily activities. Given the broad range of applications from video surveillance to human–computer interaction, scientiﬁc milestones in action recognition are achieved more rapidly, eventually leading to the demise of what used to be good in a short time. This motivated us to provide a comprehensive review of the notable steps taken towards recognizing human actions. To this end, we start our discussion with the pioneering methods that use handcrafted representations, and then, navigate into the realm of deep learning based approaches. We aim to remain objective throughout this survey, touching upon encouraging improvements as well as inevitable fallbacks, in the hope of raising fresh questions and motivating new research directions for the reader.},
	pages = {4--21},
	journaltitle = {Image and Vision Computing},
	shortjournal = {Image and Vision Computing},
	author = {Herath, Samitha and Harandi, Mehrtash and Porikli, Fatih},
	urldate = {2020-07-17},
	date = {2017-04},
	langid = {english},
	keywords = {done, selected, review},
	file = {Herath et al. - 2017 - Going deeper into action recognition A survey.pdf:/home/zampolo/Zotero/storage/HQ9WA5GR/Herath et al. - 2017 - Going deeper into action recognition A survey.pdf:application/pdf}
}

@Article{xia-2020,
  author       = {Xia, Kun and Huang, Jianguang and Wang, Hanyu},
  title        = {{LSTM}-{CNN} Architecture for Human Activity Recognition},
  volume       = {8},
  pages        = {56855--56866},
  issn         = {2169-3536},
  abstract     = {In the past years, traditional pattern recognition methods have made great progress. However, these methods rely heavily on manual feature extraction, which may hinder the generalization model performance. With the increasing popularity and success of deep learning methods, using these techniques to recognize human actions in mobile and wearable computing scenarios has attracted widespread attention. In this paper, a deep neural network that combines convolutional layers with long short-term memory ({LSTM}) was proposed. This model could extract activity features automatically and classify them with a few model parameters. {LSTM} is a variant of the recurrent neural network ({RNN}), which is more suitable for processing temporal sequences. In the proposed architecture, the raw data collected by mobile sensors was fed into a two-layer {LSTM} followed by convolutional layers. In addition, a global average pooling layer ({GAP}) was applied to replace the fully connected layer after convolution for reducing model parameters. Moreover, a batch normalization layer ({BN}) was added after the {GAP} layer to speed up the convergence, and obvious results were achieved. The model performance was evaluated on three public datasets ({UCI}, {WISDM}, and {OPPORTUNITY}). Finally, the overall accuracy of the model in the {UCI}-{HAR} dataset is 95.78\%, in the {WISDM} dataset is 95.85\%, and in the {OPPORTUNITY} dataset is 92.63\%. The results show that the proposed model has higher robustness and better activity detection capability than some of the reported results. It can not only adaptively extract activity features, but also has fewer parameters and higher accuracy.},
  date         = {2020},
  doi          = {10.1109/ACCESS.2020.2982225},
  file         = {09043535.pdf:/home/zampolo/engenharia/artigos/activity-recognition/09043535.pdf:application/pdf},
  journaltitle = {{IEEE} Access},
  keywords     = {to read},
  langid       = {english},
  shortjournal = {{IEEE} Access},
  url          = {https://ieeexplore.ieee.org/document/9043535/},
  urldate      = {2020-06-24},
}

@Article{hochreiter-1997,
  author   = {Hochreiter, Sepp and Schmidhuber, Jurgen},
  title    = {Long short-term memory.},
  journal  = {Neural Computation},
  year     = {1997},
  volume   = {9},
  number   = {8},
  pages    = {1735},
  issn     = {08997667},
  abstract = {Addresses the problem of storing information over extended time intervals by recurrent backpropagation. Introduction of a novel, efficient, gradient-based method called long short-term memory; Results of experiments with artificial data.},
  keywords = {Information retrieval, Back propagation, Short-term memory},
  url      = {http://search-ebscohost-com.ez3.periodicos.capes.gov.br/login.aspx?direct=true&db=iih&AN=9710215021&lang=pt-br&site=ehost-live&authtype=ip,cookie,uid},
}

@Article{donahue-2016,
  author       = {Donahue, Jeff and Hendricks, Lisa Anne and Rohrbach, Marcus and Venugopalan, Subhashini and Guadarrama, Sergio and Saenko, Kate and Darrell, Trevor},
  title        = {Long-term Recurrent Convolutional Networks for Visual Recognition and Description},
  abstract     = {Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent are effective for tasks involving sequences, visual and otherwise. We describe a class of recurrent convolutional architectures which is end-to-end trainable and suitable for large-scale visual understanding tasks, and demonstrate the value of these models for activity recognition, image captioning, and video description. In contrast to previous models which assume a ﬁxed visual representation or perform simple temporal averaging for sequential processing, recurrent convolutional models are “doubly deep” in that they learn compositional representations in space and time. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Differentiable recurrent models are appealing in that they can directly map variable-length inputs (e.g., videos) to variable-length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent sequence models are directly connected to modern visual convolutional network models and can be jointly trained to learn temporal dynamics and convolutional perceptual representations. Our results show that such models have distinct advantages over state-of-the-art models for recognition or generation which are separately deﬁned or optimized.},
  date         = {2016-05-31},
  eprint       = {1411.4389},
  eprinttype   = {arxiv},
  file         = {1411.4389.pdf:/home/zampolo/engenharia/artigos/activity-recognition/1411.4389.pdf:application/pdf},
  journaltitle = {{arXiv}:1411.4389 [cs]},
  keywords     = {to read},
  langid       = {english},
  url          = {http://arxiv.org/abs/1411.4389},
  urldate      = {2020-06-24},
}

@Article{wang-2015,
  author       = {Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu},
  title        = {Towards Good Practices for Very Deep Two-Stream {ConvNets}},
  abstract     = {Deep convolutional networks have achieved great success for object recognition in still images. However, for action recognition in videos, the improvement of deep convolutional networks is not so evident. We argue that there are two reasons that could probably explain this result. First the current network architectures (e.g. Two-stream {ConvNets} [12]) are relatively shallow compared with those very deep models in image domain (e.g. {VGGNet} [13], {GoogLeNet} [15]), and therefore their modeling capacity is constrained by their depth. Second, probably more importantly, the training dataset of action recognition is extremely small compared with the {ImageNet} dataset, and thus it will be easy to over-ﬁt on the training dataset.},
  date         = {2015-07-08},
  eprint       = {1507.02159},
  eprinttype   = {arxiv},
  file         = {Wang et al. - 2015 - Towards Good Practices for Very Deep Two-Stream Co.pdf:/home/zampolo/Zotero/storage/GMQP356M/Wang et al. - 2015 - Towards Good Practices for Very Deep Two-Stream Co.pdf:application/pdf},
  journaltitle = {{arXiv}:1507.02159 [cs]},
  keywords     = {Computer Science - Computer Vision and Pattern Recognition},
  langid       = {english},
  url          = {http://arxiv.org/abs/1507.02159},
  urldate      = {2020-08-29},
}

@Article{cao-2019,
  author  = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},
  title   = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year    = {2019},
}

@Comment{jabref-meta: databaseType:bibtex;}
